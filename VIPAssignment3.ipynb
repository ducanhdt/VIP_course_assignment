{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ps_utils\n",
    "import numpy.linalg as la\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Beethoven Dataset \n",
    "Beethoven is a synthetic and clean dataset, with exactly 3 images. If nz is the number of pixels inside the non-zero part of the mask, You should create an array J of size/shape (3, nz) and obtain the albedo modulated normal field as $M = S^{-1}J$. With it, extract the albedo within the mask, display it as a 2D image. Then extract the normal field by normalizing M, extract its components n1, n2, n3. Solve for depth and display it at different view points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# we don't have to include this block, I just wrote it so I can see the picture in original datasets\n",
    "def display_original_imageset(filename):\n",
    "    data = loadmat(filename)\n",
    "    img = data['I']\n",
    "    \n",
    "    num_rows = math.ceil(img.shape[2] / 3)\n",
    "    fig, axs = plt.subplots(num_rows, 3, figsize=(10, num_rows * 5))\n",
    "    \n",
    "    for i in range(img.shape[2]):\n",
    "        row = i // 3  \n",
    "        col = i % 3   \n",
    "        ax = axs.flat[i]\n",
    "        ax.imshow(img[:,:,i])\n",
    "        ax.axis('off')  \n",
    "    \n",
    "    for i in range(img.shape[2], num_rows * 3):\n",
    "        fig.delaxes(axs.flatten()[i])\n",
    "\n",
    "    plt.tight_layout() \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_original_imageset('Beethoven.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def woodham_depth_solution(dataset, depth_map_func):\n",
    "    I, mask, S = ps_utils.read_data_file(dataset)\n",
    "    nz = np.where(mask > 0)\n",
    "    m,n = mask.shape\n",
    "    \n",
    "    J = np.zeros((S.shape[0],len(nz[0])))\n",
    "    for i in range(S.shape[0]):\n",
    "        Ii = I[:,:,i]\n",
    "        J[i,:] = Ii[nz]\n",
    "    \n",
    "    if S.shape[0] == S.shape[1]:\n",
    "        M = np.dot(la.inv(S),J)\n",
    "    else:\n",
    "        M = np.dot(la.pinv(S),J)\n",
    "           \n",
    "    albedo = la.norm(M, axis=0)  \n",
    "    albedo_image = np.zeros((m,n))\n",
    "    albedo_image[nz] = albedo\n",
    "    plt.title('Albedo')\n",
    "    plt.imshow(albedo_image)\n",
    "    plt.show()  \n",
    "    \n",
    "    N = M/albedo\n",
    "    n1 = np.zeros((m,n))\n",
    "    n2 = np.zeros((m,n))\n",
    "    n3 = np.ones((m,n))\n",
    "    n1[nz] = N[0,:]\n",
    "    n2[nz] = N[1,:]\n",
    "    n3[nz] = N[2,:]\n",
    "    \n",
    "    fig,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(10,5))\n",
    "    ax1.imshow(n1)\n",
    "    ax2.imshow(n2)\n",
    "    ax3.imshow(n3)\n",
    "    fig.subplots_adjust(wspace=0.4)\n",
    "    plt.show()\n",
    "    \n",
    "    z = depth_map_func(n1, n2, n3, mask)\n",
    "    z = np.nan_to_num(z)\n",
    "    ps_utils.display_surface(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woodham_depth_solution('Beethoven', ps_utils.simchony_integrate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 mat_vase Dataset\n",
    "mat vase is a synthetic and clean dataset, with exactly 3 images. If nz is the number of pixels inside the non-zero part of the mask, You should create an array J of size/shape (3, nz) and obtain the albedo modulated normal field as M = $S^{-1}J$. With it, extract the albedo within the mask, display it as a 2D image. Then extract the normal field by normalizing M, extract its components n1, n2, n3. Solve for depth and display it at different view points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woodham_depth_solution('mat_vase', ps_utils.simchony_integrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woodham_depth_solution('mat_vase', ps_utils.unbiased_integrate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 shiny_vase Dataset\n",
    "shiny vase is a synthetic and clean dataset, however not respecting Lambert’s law, which cannot cope with specularities. It also consists of exactly 3 images. If nz is the number of pixels inside the non-zero part of the mask, You should create an array J of size/shape (3, nz) and obtain the albedo modulated normal field as M = $S^{-1}J$. With it, extract the albedo within the mask, display it as a 2D image. Then extract the normal field by normalizing M, extract its components n1, n2, n3. Solve for depth and display it at different view points. Comment on what happens here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woodham_depth_solution('shiny_vase', ps_utils.unbiased_integrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woodham_depth_solution('shiny_vase', ps_utils.simchony_integrate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think that RANSAC could provide a better estimation of normals? Explain. You should try and replace Woodham’s first step (via inverse/pseudoinverse) with RANSAC estimation. The threshold parameter in ransac 3dvector() should no more than be 2.0. After the estimation for each pixel, extract the normals and the albedo. Display and comment on the results. Do they differ from Woodham’s estimation? Try then to make the estimated normal field smoother using the smooth normal field() function. You may experiment with the iters parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Ransac_depth_solution(dataset, depth_map_func, threshold=2,smooth = False,iters=100):\n",
    "    I, mask, S = ps_utils.read_data_file(dataset)\n",
    "    nz = np.where(mask > 0)\n",
    "    m,n = mask.shape\n",
    "    \n",
    "    J = np.zeros((S.shape[0],len(nz[0])))\n",
    "    for i in range(S.shape[0]):\n",
    "        Ii = I[:,:,i]\n",
    "        J[i,:] = Ii[nz]\n",
    "    \n",
    "    normal = np.zeros((3, len(nz[0])))\n",
    "    albedo = np.zeros(len(nz[0]))\n",
    "     \n",
    "    for i in range(len(nz[0])):\n",
    "        data = (J[:,i],S)\n",
    "        result = ps_utils.ransac_3dvector(data, threshold=threshold)\n",
    "        if result:\n",
    "            best_m,_,_ = result\n",
    "            albedo[i] = la.norm(best_m)\n",
    "            normal[:,i] = best_m/albedo[i]\n",
    "    \n",
    "    #data = (J, S)    \n",
    "    #normal = ps_utils.ransac_3dvector(data, threshold=2)\n",
    "    #albedo = la.norm(normal, axis=0)   \n",
    "    \n",
    "    n1, n2, n3 = np.zeros((m,n)),np.zeros((m,n)),np.ones((m,n))\n",
    "    n1[nz], n2[nz], n3[nz] = normal\n",
    "    if smooth:\n",
    "        n1, n2, n3 = ps_utils.smooth_normal_field(n1,n2,n3,mask,iters=iters)   \n",
    "    \n",
    "    fig,(ax1,ax2,ax3) = plt.subplots(1,3,figsize=(10,5))\n",
    "    ax1.imshow(n1)\n",
    "    ax2.imshow(n2)\n",
    "    ax3.imshow(n3)\n",
    "    if smooth:\n",
    "        fig.suptitle(f'Iters = {iters}', y=0.85)\n",
    "    fig.subplots_adjust(wspace=0.4)\n",
    "    plt.show()\n",
    "    \n",
    "    albedo_image = np.zeros((m,n))\n",
    "    albedo_image[nz] = albedo\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.imshow(albedo_image)\n",
    "    plt.show()\n",
    "\n",
    "    z = depth_map_func(n1, n2, n3, mask)\n",
    "    z = np.nan_to_num(z)\n",
    "    \n",
    "    ps_utils.display_surface(z)\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ransac_depth_solution('shiny_vase', ps_utils.simchony_integrate, threshold=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the estimated normal field smoother using the smooth normal field() function\n",
    "Ransac_depth_solution('shiny_vase', ps_utils.simchony_integrate, threshold=2, smooth = True, iters=100)\n",
    "Ransac_depth_solution('shiny_vase', ps_utils.simchony_integrate, threshold=2, smooth = True, iters=50)\n",
    "Ransac_depth_solution('shiny_vase', ps_utils.simchony_integrate, threshold=2, smooth = True, iters=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 shiny_vase2 Dataset\n",
    "shiny vase2 is a synthetic and clean dataset, however not respecting Lambert’s law, which cannot cope with specularities. It consists of exactly 22 images. If nz is the number of pixels inside the non-zero part of the mask, You should create an array J of size/shape (22,nz) and obtain the albedo modulated normal field as M = S†J. With it, extract the albedo within the mask, display it as a 2D image. Then extract the normal field by normalizing M, extract its components n1, n2, n3. Solve for depth and display it at different view points. Comment on what happens here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woodham_depth_solution('shiny_vase2', ps_utils.simchony_integrate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think that RANSAC could provide a better estimation of normals? Explain. You should try and replace Woodham’s first step (via inverse/pseudoinverse) with RANSAC estimation. The threshold parameter in ransac 3dvector() should no more than be 2.0. After the estimation for each pixel, extract the normals and the albedo. Display and comment on the results. Do they differ from Woodham’s estimation? Try then to make the estimated normal field smoother using the smooth normal field() function. You may experiment with the iters parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ransac_depth_solution('shiny_vase2', ps_utils.simchony_integrate, threshold=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ransac_depth_solution('shiny_vase2', ps_utils.simchony_integrate, threshold=2, smooth = True, iters=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ransac_depth_solution('shiny_vase2', ps_utils.simchony_integrate, threshold=2, smooth = True, iters=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 face Dataset\n",
    "face is real dataset, with exactly 10 images. If nz is the number of pixels inside the non-zero part of the mask, You should create an array J of size/shape (10,nz) and obtain the albedo modulated normal field as M = S†J (the pseudo-inverse). With it, extract the albedo within the mask, display it as a 2D image. Then extract the normal field by normalizing M, extract its components n1, n2, n3. Solve for depth and display it at different view points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woodham_depth_solution('Buddha', ps_utils.simchony_integrate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should try and replace Woodham’s first step (via inverse/pseudoinverse) with an estimation using RANSAC. The threshold parameter in ransac 3dvector() should be at least 25.0 Experiment with it. After the estimation for each pixel, extract the normals and the albedo. Display and comment on the results. Try then to make the estimated normal field smoother using the smooth normal field() function. You may experiment with the iters parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ransac_depth_solution('Buddha', ps_utils.unbiased_integrate, threshold=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ransac_depth_solution('Buddha', ps_utils.simchony_integrate, threshold=200, smooth = True, iters=100)\n",
    "Ransac_depth_solution('Buddha', ps_utils.simchony_integrate, threshold=200, smooth = True, iters=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 face Dataset\n",
    "Buddha is real dataset, with exactly 27 images. If nz is the number of pixels inside the non-zero part of the mask, You should create an array J of size/shape (27,nz) and obtain the albedo modulated normal field via RANSAC with a threshold of 10.0 Try then to make the estimated normal field smoother using the smooth normal field() function. You may experiment with the iters parameter. Report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ransac_depth_solution('face', ps_utils.simchony_integrate, threshold=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ransac_depth_solution('face', ps_utils.simchony_integrate, threshold=10.0, smooth = True, iters=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ransac_depth_solution('face', ps_utils.simchony_integrate, threshold=10.0, smooth = True, iters=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
