{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\Ducanh\\.cache\\kagglehub\\datasets\\imbikramsaha\\caltech-101\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "# !pip install kagglehub\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"imbikramsaha/caltech-101\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9145 102\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "path = \"caltech-101/\"\n",
    "\n",
    "image_paths = glob.glob(f\"{path}/*/*\")\n",
    "labels = [os.path.basename(os.path.dirname(image_path)) for image_path in image_paths]\n",
    "print(len(labels), len(set(labels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def sample_labels_and_images(\n",
    "    image_paths,\n",
    "    labels,\n",
    "    num_labels=10,\n",
    "    samples_per_label=10,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "):\n",
    "    unique_labels = list(set(labels))\n",
    "    selected_unique_labels = random.sample(\n",
    "        unique_labels, min(num_labels, len(unique_labels))\n",
    "    )\n",
    "\n",
    "    label_to_paths = {}\n",
    "    for path, label in zip(image_paths, labels):\n",
    "        if label in selected_unique_labels:\n",
    "            if label not in label_to_paths:\n",
    "                label_to_paths[label] = []\n",
    "            label_to_paths[label].append(path)\n",
    "\n",
    "    sampled_image_paths = []\n",
    "    sampled_labels = []\n",
    "\n",
    "    for label, paths in label_to_paths.items():\n",
    "        sample_count = min(samples_per_label, len(paths))\n",
    "        sampled_paths = random.sample(paths, sample_count)\n",
    "        sampled_image_paths.extend(sampled_paths)\n",
    "        sampled_labels.extend([label] * sample_count)\n",
    "\n",
    "    train_image_paths, test_image_paths, train_labels, test_labels = train_test_split(\n",
    "        sampled_image_paths,\n",
    "        sampled_labels,\n",
    "        test_size=test_size,\n",
    "        stratify=sampled_labels,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        train_image_paths,\n",
    "        train_labels,\n",
    "        test_image_paths,\n",
    "        test_labels,\n",
    "        selected_unique_labels,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def top_k_accuracy(y_true, y_top_k_predict,k=3):\n",
    "    correct_predictions = sum(\n",
    "        1 for true, top_k in zip(y_true, y_top_k_predict) \n",
    "        if true in top_k[:k]\n",
    "    )\n",
    "    accuracy = correct_predictions / len(y_true) * 100\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "def mean_reciprocal_rank(y_true, y_top_k_predict):\n",
    "    reciprocal_ranks = []\n",
    "    for true_label, top_k_preds in zip(y_true, y_top_k_predict):\n",
    "        # Find the rank of the true label in predictions\n",
    "        try:\n",
    "            rank = np.where(top_k_preds==true_label)[0][0]+1\n",
    "            reciprocal_ranks.append(1 / rank)\n",
    "        except:\n",
    "            # True label not found in predictions\n",
    "            reciprocal_ranks.append(0)\n",
    "    \n",
    "    # Calculate Mean Reciprocal Rank\n",
    "    mrr = sum(reciprocal_ranks) / len(reciprocal_ranks)\n",
    "    \n",
    "    return mrr\n",
    "\n",
    "class CookBook:\n",
    "    def __init__(\n",
    "        self, train_paths, train_labels, test_paths, test_labels, n_cluster=100\n",
    "    ):\n",
    "        self.train_paths = train_paths\n",
    "        self.train_labels = np.array(train_labels)\n",
    "        self.test_paths = test_paths\n",
    "        self.test_labels = np.array(test_labels)\n",
    "        self.n_clusters = n_cluster\n",
    "        self.train_features = None\n",
    "        self.test_features = None\n",
    "        self.train()\n",
    "\n",
    "    def extract_sift_features(self, image_paths):\n",
    "        if type(image_paths) == str:\n",
    "            image_paths = [image_paths]\n",
    "\n",
    "        sift = cv2.SIFT_create()\n",
    "\n",
    "        # Lists to store descriptors\n",
    "        all_descriptors = []\n",
    "        feature_counts = []\n",
    "\n",
    "        # Extract SIFT features from each image\n",
    "        for path in tqdm(image_paths, desc=\"Extracting SIFT Features\"):\n",
    "            img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "            _, descriptors = sift.detectAndCompute(img, None)\n",
    "\n",
    "            # If descriptors are found, add them to the list\n",
    "            if descriptors is not None:\n",
    "                all_descriptors.append(descriptors)\n",
    "                feature_counts.append(len(descriptors))\n",
    "\n",
    "        return all_descriptors, feature_counts\n",
    "\n",
    "    def train(self):\n",
    "        start = time.time()\n",
    "        self.descriptors, _ = self.extract_sift_features(self.train_paths)\n",
    "        descriptors_stack = np.vstack(self.descriptors)\n",
    "        print(\"Kmean clustering...\")\n",
    "        self.kmeans = KMeans(n_clusters=self.n_clusters, random_state=42, n_init=10)\n",
    "        self.kmeans.fit(descriptors_stack)\n",
    "\n",
    "        bow_features = []\n",
    "        for descriptors in self.descriptors:\n",
    "            # If no descriptors, return zero vector\n",
    "            if descriptors is None or len(descriptors) == 0:\n",
    "                return np.zeros(self.n_clusters)\n",
    "\n",
    "            # Assign descriptors to nearest visual words\n",
    "            visual_words = self.kmeans.predict(descriptors)\n",
    "\n",
    "            # Compute histogram\n",
    "            histogram, _ = np.histogram(visual_words, bins=range(self.n_clusters + 1))\n",
    "            bow_features.append(histogram)\n",
    "        self.train_features = bow_features\n",
    "        \n",
    "        self.tfidf_transformer = TfidfTransformer()\n",
    "        self.train_features_tfidf = self.tfidf_transformer.fit_transform(self.train_features)\n",
    "\n",
    "\n",
    "        self.train_probs = np.array([hist / np.sum(hist) for hist in self.train_features])\n",
    "        self.train_probs = np.clip(self.train_probs, 1e-10, None)\n",
    "        print(\"Total Training Time:\", time.time()-start)\n",
    "        \n",
    "    def indexing(self, image_paths):\n",
    "        if not self.kmeans:\n",
    "            raise Exception(\"model haven't train\")\n",
    "\n",
    "        all_descriptors, _ = self.extract_sift_features(image_paths)\n",
    "\n",
    "        bow_features = []\n",
    "        for descriptors in all_descriptors:\n",
    "            # If no descriptors, return zero vector\n",
    "            if descriptors is None or len(descriptors) == 0:\n",
    "                return np.zeros(self.n_clusters)\n",
    "\n",
    "            # Assign descriptors to nearest visual words\n",
    "            visual_words = self.kmeans.predict(descriptors)\n",
    "\n",
    "            # Compute histogram\n",
    "            histogram, _ = np.histogram(visual_words, bins=range(self.n_clusters + 1))\n",
    "            bow_features.append(histogram)\n",
    "        return np.array(bow_features)\n",
    "\n",
    "    def common_word_retrieval(self, query, k=10):\n",
    "        common_words = np.minimum(query, self.train_features).sum(axis=1)\n",
    "        top_indices = np.argsort(common_words)[::-1][:k]\n",
    "        top_labels = self.train_labels[top_indices]\n",
    "\n",
    "        return top_labels\n",
    "\n",
    "    def tfidf_retrieval(self, query, k=10):\n",
    "        query_tfidf = self.tfidf_transformer.transform(query.reshape(1, -1))\n",
    "        similarities = cosine_similarity(query_tfidf, self.train_features_tfidf).flatten()\n",
    "        top_indices = np.argsort(similarities)[::-1][:k]\n",
    "        top_labels = self.train_labels[top_indices]\n",
    "\n",
    "        return top_labels  \n",
    "      \n",
    "    def KL_divergence_retrieval(self, query, k=10):\n",
    "        query_prob = query / np.sum(query)\n",
    "        \n",
    "        query_prob = np.clip(query_prob, 1e-10, None)\n",
    "\n",
    "        # Compute KL divergence for all training histograms simultaneously\n",
    "        kl_divergences = np.sum(query_prob * np.log(query_prob / self.train_probs), axis=1)\n",
    "        \n",
    "        # Get the indices of the top-k smallest KL divergences\n",
    "        top_indices = np.argsort(kl_divergences)[:k]\n",
    "        \n",
    "        # Retrieve corresponding labels\n",
    "        top_labels = self.train_labels[top_indices]\n",
    "        \n",
    "        return top_labels\n",
    "\n",
    "    def evaluate(self, test=\"common_word_retrieval\"):\n",
    "\n",
    "        if self.test_features is None:\n",
    "            self.test_features = self.indexing(self.test_paths)\n",
    "        start = time.time()\n",
    "        if test ==\"common_word_retrieval\":\n",
    "            self.test_retrival_results = np.array([self.common_word_retrieval(query) for query in self.test_features])\n",
    "            self.train_retrival_results = np.array([self.common_word_retrieval(query) for query in self.train_features])\n",
    "        \n",
    "        if test ==\"tfidf_retrieval\":\n",
    "            self.test_retrival_results = np.array([self.tfidf_retrieval(query) for query in self.test_features])\n",
    "            self.train_retrival_results = np.array([self.tfidf_retrieval(query) for query in self.train_features])    \n",
    "        \n",
    "        if test == \"KL_divergence_retrieval\":\n",
    "            self.test_retrival_results = np.array([self.KL_divergence_retrieval(query) for query in self.test_features])\n",
    "            self.train_retrival_results = np.array([self.KL_divergence_retrieval(query) for query in self.train_features])    \n",
    "        running_time = time.time() - start\n",
    "        print(test)\n",
    "        print(\"n_cluster = \", self.n_clusters)\n",
    "        print(\"Train top 3 accuracy:\", top_k_accuracy(self.train_labels, self.train_retrival_results))\n",
    "        print(\"Test top 3 accuracy:\", top_k_accuracy(self.test_labels, self.test_retrival_results))\n",
    "        \n",
    "        print(\"Train mean reciprocal rank:\", mean_reciprocal_rank(self.train_labels, self.train_retrival_results))\n",
    "        print(\"Test mean reciprocal rank:\", mean_reciprocal_rank(self.test_labels, self.test_retrival_results))\n",
    "        print(\"Total Runing Time: \",running_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Labels: ['electric_guitar', 'hedgehog', 'platypus', 'lobster', 'mandolin', 'tick', 'pagoda', 'butterfly', 'panda', 'crocodile_head', 'revolver', 'flamingo', 'watch', 'cellphone', 'cannon', 'schooner', 'pigeon', 'Leopards', 'headphone', 'dolphin']\n",
      "\n",
      "Training Set:\n",
      "Total train images: 951\n",
      "Train samples per label:\n",
      "Counter({'Leopards': 80, 'watch': 80, 'butterfly': 73, 'revolver': 66, 'electric_guitar': 60, 'flamingo': 54, 'dolphin': 52, 'schooner': 50, 'cellphone': 47, 'hedgehog': 43, 'crocodile_head': 41, 'tick': 39, 'pagoda': 38, 'pigeon': 36, 'mandolin': 34, 'headphone': 34, 'cannon': 34, 'lobster': 33, 'panda': 30, 'platypus': 27})\n",
      "\n",
      "Testing Set:\n",
      "Total test images: 238\n",
      "Test samples per label:\n",
      "Counter({'Leopards': 20, 'watch': 20, 'butterfly': 18, 'revolver': 16, 'electric_guitar': 15, 'flamingo': 13, 'schooner': 13, 'dolphin': 13, 'cellphone': 12, 'hedgehog': 11, 'crocodile_head': 10, 'tick': 10, 'mandolin': 9, 'pigeon': 9, 'pagoda': 9, 'cannon': 9, 'lobster': 8, 'panda': 8, 'headphone': 8, 'platypus': 7})\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "train_paths, train_labels, test_paths, test_labels, selected_labels = sample_labels_and_images(\n",
    "    image_paths, labels, num_labels=20, samples_per_label=100, test_size=0.2\n",
    ")\n",
    "\n",
    "# Print out statistics\n",
    "print(\"Selected Labels:\", selected_labels)\n",
    "print(\"\\nTraining Set:\")\n",
    "print(f\"Total train images: {len(train_paths)}\")\n",
    "from collections import Counter\n",
    "print(\"Train samples per label:\")\n",
    "print(Counter(train_labels))\n",
    "\n",
    "print(\"\\nTesting Set:\")\n",
    "print(f\"Total test images: {len(test_paths)}\")\n",
    "print(\"Test samples per label:\")\n",
    "print(Counter(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting SIFT Features: 100%|██████████| 951/951 [01:09<00:00, 13.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmean clustering...\n",
      "Total Training Time: 641.5259337425232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting SIFT Features: 100%|██████████| 238/238 [00:15<00:00, 15.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common_word_retrieval\n",
      "n_cluster =  50\n",
      "Train top 3 accuracy: 77.60252365930599\n",
      "Test top 3 accuracy: 25.630252100840334\n",
      "Train mean reciprocal rank: 0.710340410261546\n",
      "Test mean reciprocal rank: 0.19452781112444986\n",
      "Total Runing Time:  1.5982460975646973\n",
      "tfidf_retrieval\n",
      "n_cluster =  50\n",
      "Train top 3 accuracy: 100.0\n",
      "Test top 3 accuracy: 50.84033613445378\n",
      "Train mean reciprocal rank: 1.0\n",
      "Test mean reciprocal rank: 0.42880985727624377\n",
      "Total Runing Time:  6.836222171783447\n",
      "KL_divergence_retrieval\n",
      "n_cluster =  50\n",
      "Train top 3 accuracy: 100.0\n",
      "Test top 3 accuracy: 46.21848739495798\n",
      "Train mean reciprocal rank: 1.0\n",
      "Test mean reciprocal rank: 0.39039949313058575\n",
      "Total Runing Time:  2.6312737464904785\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cookbook = CookBook(train_paths, train_labels, test_paths, test_labels, n_cluster=50)\n",
    "cookbook.evaluate(\"common_word_retrieval\")\n",
    "cookbook.evaluate(\"tfidf_retrieval\")\n",
    "cookbook.evaluate(\"KL_divergence_retrieval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
